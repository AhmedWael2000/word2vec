# word2vec
Word embedding is a technique in natural language processing that represents words as numerical vectors in a multi-dimensional space. It's used to capture semantic relationships between words and their contexts in a way that machine learning algorithms can understand.
